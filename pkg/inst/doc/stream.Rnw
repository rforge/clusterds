%\documentclass[10pt,a4paper]{article}
\documentclass[nojss]{jss}

\usepackage[utf8]{inputenc}

%\usepackage{a4wide} 
%\setlength{\parskip}{0.5ex plus0.1ex minus0.1ex}
%\setlength{\parindent}{0em}

%\usepackage[round,longnamesfirst]{natbib} 
\usepackage{hyperref}

%%% for tabulars
\usepackage{rotating}
\usepackage{multirow}

%%% for hanging paragraph
\usepackage{hanging}

%%% double spacing 
% \usepackage{setspace} 
% \doublespacing

%\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
%\newcommand{\code}[1]{\mbox{\texttt{#1}}} \newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
%\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\vect}[1]{\mathbf{#1}}

%\usepackage{Sweave} 
%\VignetteIndexEntry{Introduction to stream}


\author{John Forrest\\Southern Methodist University 
\And Michael Hahsler\\Southern Methodist University
\And Matthew Bolanos\\Southern Methodist University} 

\title{Introduction to \pkg{stream}: A Framework for Data Stream Modeling in R}

\Plainauthor{John Forrest, Michael Hahsler, Matthew Bolanos} 
\Plaintitle{Introduction to stream: A Framework for Data Stream Modeling in R}
\Shorttitle{Introduction to stream}

%% an abstract and keywords
\Abstract{ In recent years, data streams have become an increasingly important
area of research. Common data mining tasks associated with data streams include
classification and clustering. Due to both the size and the dynamic nature of
data streams, it is often difficult to obtain real-time stream data without the
overhead of setting up an infrastructure that will generate data with specific
properties. We have built the framework in \proglang{R}, a popular tool for
data mining and statistical analysis with the intent that researchers will be
able to easily integrate our framework into their existing work. In this paper
we introduce the implementation of \pkg{stream}, an \proglang{R} package that
provides an intuitive interface for experimenting on data streams and their
applications. \pkg{stream} is a general purpose tool that can model data
streams and perform data mining tasks on the generated data. It allows the
researcher to control specific behaviors of the streams so that they create
scenarios that may not be easily reproducible in the real-world, such as the
merging and splitting of clusters. Additionally, it has the ability to replay
the requested data for other data mining tasks if needed, or read data streams
from other sources and incorporate them into the framework.  } 

\Keywords{data stream, data mining, cluster, classification} 
\Plainkeywords{data stream, data mining, cluster, classification} 

\Address{ Michael Hahsler\\ 
Computer Science and Engineering\\ 
Lyle School of Engineering\\ 
Southern Methodist University\\ 
P.O. Box 750122 \\ 
Dallas, TX 75275-0122\\ 
E-mail: \email{mhahsler@lyle.smu.edu}\\ 
URL: \url{http://lyle.smu.edu/~mhahsler}

John Forrest\\
Microsoft Corporation\\
E-mail: \email{jforrest@microsoft.com}

Matthew Bolanos\\
Southern Methodist University\\
E-mail: \email{mbolanos@smu.edu}
}

\begin{document}
\vfill
\section*{Acknowledgments} This  work is supported in part by the U.S. National
Science Foundation as a research experience for undergraduates (REU) under
contract number IIS-0948893.

%\maketitle

%% Add TOC
\clearpage \tableofcontents \clearpage

\section{Introduction}
Typical statistical and data mining methods (e.g.,
parameter estimation, statistical tests,
clustering, classification and frequent pattern analysis)
work with ``static'' data sets, meaning that the complete data set is
available as a whole to perform all necessary 
computations.
Well known methods like $k$-means clustering, decision tree induction and
the apriori algorithm to find frequent itemsets scan the complete 
data set repeatedly to produce 
their results~\citep{stream:Hastie+Tibshirani+Friedman:2001}. 
However, in recent years more and more applications need to work with data
which are not static, but the result of a 
continuous data generation process which even might evolve over time. 
Some examples are web click-stream
data, computer network monitoring data, telecommunication connection data,
readings from sensor nets and stock quotes.
This type of data is called data stream and it has become 
an increasingly important area of
research~\citep{stream:Babcock:2002,stream:Gaber:2005,stream:Aggarwal:2007}.  
Early on also the statistical community started to see the emerging field
of statistical analysis of massive data streams (see~\cite{stream:NRC:2004}).

A data stream can be formalized as an ordered sequence of data points 
$$\langle \vect{y}_1, \vect{y}_2, \vect{y}_3, \ldots\rangle,$$
where the index reflects the order (either by explicit time 
stamps or just by an integer reflecting order).
The data points themselves can be simple vectors in multidimensional space, 
but can also contains nominal/ordinal variables, complex information
(e.g., graphs) or unstructured information (e.g., text).
The characteristic of continually arriving data points introduces an important
property of data streams which also poses the greatest challenge: the size
of a data stream is unbounded. This leads to the following 
requirements for data stream processing algorithms:

\begin{itemize} 
\item \textbf{Bounded storage:} The algorithm can only store a
very limited amount of data to summarize the data stream. 
\item \textbf{Single pass:} The incoming
data points cannot be permanently stored and need to be processed at once in
the arriving order.  
\item \textbf{Real-time:} The algorithm has to process data points on
average at least as fast as the arriving data.  
\item \textbf{Concept drift:}
The algorithm has be able to deal with a data generation process which evolves
over time (e.g., distributions change or new structure in the data appears).  
\end{itemize}

Obviously, most existing algorithms designed for static data are not 
able to satisfy these requirements and thus are only usable if
techniques like sampling or time windows are used to extract small,
quasi-static subsets. 
Although these approaches are important, 
new algorithms are needed and have been introduced over the last decade 
to deal with the special challenges posed by data streams.

Although R is an ideal platform to develop and test prototypes
for data stream algorithms, currently R does not have an infrastructure 
to support data streams: 
\begin{enumerate}
\item Data sets are typically represented
by data.frames or matrices which is suitable for static data but not to represent streams. 
\item Algorithms for data streams are not available in R.
\end{enumerate}

In this paper we introduce the package~\pkg{stream} 
which provides a framework to represent and process data streams 
and use them to develop, test and compare data stream algorithms in R.
We include an initial set of 
data stream generators and data stream algorithms (focusing
on clustering) in this package with
the hope that other researchers will contribute their algorithms and
ultimately use \pkg{stream} to develop, study and improve their algorithms.


The paper is organized as follows... 
%%%

\section{Data Stream Algorithms} \label{sec:mining}

Due to advances in data gathering techniques, it is often the case that data is
no longer viewed as a static collection, but rather as a dynamic set, or
stream, of incoming data points. 
The most common data
stream mining tasks are clustering, classification and frequent pattern
mining~\citep{stream:Aggarwal:2007}. 
The rest of this section will introduce these data stream mining tasks 
with a focus on clustering.


\subsection{Clustering} \label{sec:background:dsc}

Clustering, the assignment of data points to $k$ groups
such that point within each group are more similar than points in different 
groups is a very basic unsupervised data mining task. For 
static data sets methods like $k$-means, $k$-medians, 
hierarchical clustering and density-based methods 
have been developed among others. However, the standard algorithms for these
methods need access to 
all data points and this typically multiple times. This requirement makes
these algorithms unsuitable for data streams and led to the 
development of data stream clustering algorithms.

A first data stream clustering algorithm called \emph{STREAM} was proposed by
\cite{stream_clust:O'Callaghan:2002}~\citep[see also][]{stream_clust:Guha:2003}.The algorithm attacks the $k$-medians
problem by dividing the data stream into pieces, clusters each piece
individually and then iteratively reclusters the resulting centers to obtain a
final clustering.

Starting with \emph{CluStream}~\citep{stream_clust:Aggarwal:2003}
most data stream clustering algorithms started to 
split the clustering process into two parts. 
An online component which aggregates the 
data stream in real-time into summaries often called micro-clusters
(an extension of cluster feature vectors used
            by BIRCH~\citep{stream_clust:Zhang:1996})
and
an offline component which uses only the summaries to create a final clustering.
The offline component is typically only executed on demand and uses
traditional clustering
algorithms, such as $k$-means or the density-based method~\emph{DBSCAN}.
Summarizing the
incoming data points into micro-clusters ensures that the input to the offline
component is constrained to a finite space.
To maintain a finite number of micro-clusters, a pruning function is often
associated within the summarization process. The goal of the pruning process is
to discard micro-clusters that have not enough data points assigned to them
or became obsolete.
The latter case occurs when the structure of
the data stream changes over time which is known as concept drift
\citep{stream:Masud+Chen+Khan+Aggarwal+Gao+Han+Thuraisingham:2010}.
%% FIXME: check reference

In CluStream~\citep{stream_clust:Aggarwal:2003} micro-clusters can be deleted
and merged and permanently stored at different points in time to allow to
create final clusterings (recluster micro-clusters with $k$-means) for
different time frames.  
\cite{stream_clust:Kriegel:2003} and
\cite{stream_clust:Tasoulis:2007} present variants of the density based method 
{\em OPTICS}~\citep{stream_clust:Ankerst:1999} suitable for streaming data.
\cite{stream_clust:Aggarwal:2004} introduce {\em HPStream} which finds 
clusters that are well defined in different subsets of the dimensions
of the data. The set of dimensions for each cluster can evolve over time 
and a fading function is used to discount the influence of older data points
by fading the entire cluster structure.
\cite{stream_clust:Cao:2006} introduce {\em DenStream} which maintains 
micro-clusters in real time and uses a variant of 
GDBSCAN~\citep{stream_clust:Sander:1998} to produce a final clustering 
for users.
\cite{stream_clust:Tasoulis:2006} present {\em WSTREAM,} which uses 
kernel density estimation to find rectangular windows to represent clusters.
The windows can move, contract, expand and be merged over time. 
More recent density-based data stream clustering algorithms are
{\em D-Stream}~\citep{stream_clust:Tu:2009} and 
{\em MR-Stream}~\citep{stream_clust:Wan:2009}.
{\em D-Stream} uses an online 
component to map each data point into a predefined grid and then uses an 
offline component to cluster the grid based on density.
{\em MR-Stream} facilitates the discovery of clusters
at multiple resolutions by using a
grid of cells that can dynamically be sub-divided into more cells using a tree
data structure.

%\citep{stream:Aggarwal:2009}, threshold Nearest Neighbor (tNN)

%One of the most challenging aspects of clustering is how to evaluate how well
%an algorithm has performed. There are a number of metrics used to measure the
%performance of traditional clustering algorithms
%\citep{stream:Manning+Raghavan+Schtze:2008}, but they are often used as an
%estimate of the performance rather than a guaranteed figure. Many of the
%available metrics require comparison to a true classification of the data so
%that it can be determined if incoming data points are being clustered into the
%appropriate groups. Common metrics include purity, precision, recall, entropy,
%etc. The MOA framework uses many of these traditional clustering metrics, and
%additional stream clustering metrics to evaluate the performance on stream
%clustering algorithms.


%In \pkg{stream}, our goal with data stream clustering is to separate the online
%component from each data stream clustering algorithm and use it as its own
%entity. We can then compare the performance of the online components of each
%algorithm when paired with a selected offline component. This is a feature
%unique to the \pkg{stream} framework. We focus on the online component of the
%algorithms because \proglang{R} already contains definitions for many of the
%offline components used, and the novelty of many of the algorithms is in the
%online component. Section~\ref{sec:design} discusses what data stream
%clustering algorithms are currently available in the framework, and how they
%can be operated upon.

\subsection{Classification} \label{sec:background:dscl}

Classification, learning a model in order to assign labels to new, 
unlabeled data 
points is a well studied supervised machine learning task.
Methods include naive Bayes, $k$ nearest neighbors, 
classification trees, support vector machines, rule-based classifiers 
and many more~\citep{stream:Hastie+Tibshirani+Friedman:2001}. However,
as with clustering these algorithms
need multiple access to all the training 
data and thus are not suitable for data streams with constantly arriving new
training data. 

Several classification methods suitable for data streams have 
been developed recently.
Examples are 
\emph{Very Fast Decision Trees (VFDT)}~\citep{stream:Domingos:2000}
using Hoeffding trees,
the time window-based \emph{Online Information Network 
(OLIN)}~\citep{stream:Last:2002} and
\emph{on-demand classification}~\citep{stream:Aggarwal:2004} 
based on micro-clusters found with
the data-stream clustering algorithm 
CluStream.
For a detailed description of these and other methods we refer the reader 
to the survey by~\cite{stream:Gaber:2007}

%\cite{stream:Last:2002} introduces \emph{OLIN,} an online classification
%system, which instead of all data only uses a training window with the most
%recent data to learn a classifier. The size of the training window and the
%frequency of creating a new classification model are adjusted to compensate for
%the current rate of concept drift. Since OLIN only requires the
%data in the current training window it can be used for data streams.

%An interesting new 
%novel class detection: www.cs.uiuc.edu/~hanj/pdf/pakdd10i\_mmasud.pdf


\subsection{Frequent Pattern Mining}

The aim of frequent pattern mining is to discover frequently 
occurring patterns (e.g., itemsets, subsequences, subtrees, subgraphs)
in large datasets. Patterns are then used to summarize the dataset and
can provide insights into the data. Although finding all frequent pattern  
is a computationally expensive task, many efficient algorithms
have been developed. Most notably the \emph{APRIORI} 
algorithm~\citep{arules:Agrawal:1993} 
for frequent itemsets. However, these algorithms use breath-first or
depth-first search strategies which results in the need to pass over the 
data several times and thus makes them unusable for the streaming case.
We refer the interested reader to the survey of frequent pattern 
mining in data streams 
by \cite{stream:Jin:2007}
which describe several algorithms for mining frequent itemsets. 

\section{Existing Solution: The MOA Framework} \label{sec:background:moa}

MOA (short for Massive Online Analysis) 
is a framework 
implemented in Java
for both stream classification and stream clustering
\citep{stream:Bifet+Holmes+Kirkby+Pfahringer:2010}. It is the first
experimental framework to provide easy access to multiple 
data stream mining algorithms, as well
as tools to generate data streams that can be used to measure 
and compare the performance
of different algorithms. 
Like WEKA~\citep{stream:Witten:2005}, 
a popular collection of machine learning algorithms,
MOA is also developed by the University of Waikato
and its
interface and workflow are similar to those of WEKA.

The workflow in MOA consists of three main steps:
\begin{enumerate}
\item
the selection of the data
stream model (also called data feeds or data generators); 
\item the selection of
the learning algorithm used on the generated data; and
\item the applied evaluation method.
\end{enumerate}

MOA uses a graphical user interface.
As the output MOA generates a report which contains the results from the data
mining task as well as the performance evaluation.
The learning algorithm and the evaluation differs depending
in the data mining task (classification or clustering).
Classification results are shown as text, while
clustering results have a visualization component that shows both the
clustering (for two-dimensional data) and the change in performance 
metrics over time.

The MOA framework is an important pioneer in experimenting
with data stream algorithms and facilitated by a flexible design (using the
Java Interface construct) users can add their own mining algorithms.
Big advantages of MOA is that it interfaces with the already established 
WEKA and that
it implements several data stream classification and 
clustering algorithms.

\section{The \pkg{stream} Framework} \label{sec:design}

A drawback of MOA for R users is that using its data stream mining
algorithms together with the advanced capabilities of
R to create artificial data and to analyze and visualize the results 
is currently only partially possible or very difficult.

The \pkg{stream} framework provides a R-based alternative to the MOA 
framework which directly (via \pkg{rJava}~\citep{stream:Urbanek:2011}) 
interfaces the algorithms already available in MOA while it also
allows to write algorithms in R or any language interfaceable by R.

The \pkg{stream} framework consists of two main components:
\begin{enumerate}
\item Data Stream Data (DSD) which manages or creates a data stream, and 
\item Data Stream Task (DST) which performs a data stream mining task.
\end{enumerate}
Figure~\ref{figure:workflow}  shows a high level view of the interaction of the
components.  We start by creating a DSD, then feed the data generated by
DSD into a DST object. Finally, we can obtain the results from the DST
object. DSTs can be any type of data streaming mining task like,
for example,
classification or clustering algorithms. 
In the following we will concentrate on clustering
since \pkg{stream} currently focuses on this type of task, but the
framework is implemented such that classification, frequent pattern mining
or any other task can easily be added.

\begin{figure} 
\centering 
\includegraphics[width=9cm]{architecture} 
\caption{A high level view of the \pkg{stream} architecture.} 
\label{figure:workflow} 
\end{figure}

We rely on object-oriented design using the S3 class system~\cite{}
to provide for each of the two core components an
lightweight interface (i.e., an abstract class) which can be easily 
implemented to create new data stream types or data stream mining algorihms. 
The design of the DSD and DSC classes will
be discussed in the follwing subsections.

\subsection{Data Stream Data (DSD)} \label{sec:design:dsd}

The first step in the \pkg{stream} workflow is to select a 
data stream implemented as a
Data Stream Data (DSD) object. This object can be a management layer on top of
a real data stream, a wrapper for data stored on disk or a generator which
simulates a data stream with know properties for controlled experiments. 
Figure~\ref{figure:dsd} shows relationship of the DSD
classes as a UML class diagram~\citep{stream:Fowler:2003}. 
All Data Stream Data classes extend the
base class \code{DSD}.
There are currently two types of \code{DSD} implementation,
classes which implement R-based data streams (\code{DSD_R})
and MOA-based stream generators (\code{DSD_MOA}).
The emphasis of the current implementation is on R-based
data streams and generators.
\pkg{stream} provides 
\begin{itemize}
\item \code{DSD_Gaussian_Static}, a DSD that generates static cluster data with a
random Gaussian distribution; 
\item \code{DSD_Wrapper}, a
DSD class that wraps local static data (e.g., a data.frame or a matrix) 
as a data stream; and
\item
\code{DSD_ReadStream}, a class designed
to read data from files or open connections.
\end{itemize}

From MOA currently only
\code{DSD_RandomRBF}, a data generator for moving clusters,
is interfaced. 
As depicted in the class diagram, 
other data steam implementations can be easily added in the future.

\begin{figure} 
\centering 
\includegraphics[width=11cm]{dsd_uml} 
\caption{DSD and its implemented subclasses.} 
\label{figure:dsd} 
\end{figure}


All \code{DSD} implementations share a simple interface consisting
of the following two functions.
\begin{enumerate}
\item A creator function. This function typically has the same name 
as the class. The list of parameters depend on the way the stream
is created. 
The most common input parameters for the creation of DSD classes are \code{k}
number of clusters (i.e. areas with high densities), 
and \code{d} number of dimensions. A full list of 
parameters can be obtained 
from the help page of each class. The result of this creator function
is an object representing the stream and its current state.
\item The function \code{get_points(x, n=1, ...)}. This function is used
to obtain the next data point (or next \code{n} data points) from the 
stream represented by object \code{x}. The data point(s) are returned
as a data.frame with each row representing a single data point.
\end{enumerate}

Next to these core functions several utility functions 
like \code{print()} and \code{write_stream()}
to save a part of a data stream to disk are provided.
%\code{write_stream()} can be used together with \code{DSD_ReadStream}
%to store a stream
Different data stream implementation might have additional functions
implemented. For example, \code{DSD_Wrapper} and \code{DSD_ReadStream}
have \code{reset()} implemented to reset the stream to its beginning.

\subsection{Data Stream Task (DST)} \label{sec:design:dst}

\begin{figure} 
\centering 
\includegraphics[width=11cm]{dst_uml} 
\caption{DST and its implemented subclasses.}
\label{figure:dst} \end{figure}

After choosing a DSD class to use as the data stream source, 
the next step in the
workflow is to define a Data Stream Task (DST). 
In \pkg{stream}, a DST refers to
any data mining task that can be applied to data streams. 
The design is flexible to allow for future extensions with even
currently unknown tasks.
Figure~\ref{figure:dst}
shows the class hierarchy for DST.
It is important to note that the concept of the DST class is merely 
for conceptual purposes, the actual implementation 
of clustering, classification or frequent pattern mining are 
typically quite different and share little.    
In the current implementation of \pkg{stream} we focus on
data stream clustering (DSC).


\subsection{Data Stream Clustering (DSC)} \label{sec:design:dsc}
Data stream clustering algorithms are implemented 
as subclasses of the DSC class. Here only the online component 
which has to be performed in real-time is implemented since the offline
component uses a traditional clustering method which can be easily performed
using R.

For DSC class the following function can be used:
\begin{itemize}
\item A creator function which creates an empty clustering.

\item
\code{cluster(dsc, dsd, n=1000)} which accepts a DSC object,
a DSD object, and the number of points
from the DSD object to cluster into the DSC object.

\item
\code{get_centers(x, ...)} which returns the centers, 
either the centroids or the medoids, of the
micro-clusters of the DSC object.

\item
\code{nclusters(x)} which returns the number of
micro-clusters in the DSC object.

\item
\code{print(x, ...)} which prints common attributes of the
DSC object. Currently it prints a small description of the underlying algorithm
and the number of micro-clusters that have been calculated.

\item
\code{plot(x, ..., method="pairs")} which plots the centers
of the micro-clusters. There are 3 available plot methods: \code{pairs},

\item
\code{plot(x, dsd)} which plots the centers of the micro-clusters overlayed
on top of the original data set.

\item
\code{plot}, or \code{pc}. \code{pairs} is the default method that produces a
matrix of scatter plots that plots the attributes against one another (this
method is only available when \code{d > 2}). \code{plot} simply takes the first
two attributes of the matrix and plots it as \code{x} and \code{y} on a scatter
plot. Lastly, \code{pc} performs Principle Component Analysis (PCA) on the data
and projects the data to a 2 dimensional plane and then plots the results.
\end{itemize}

Figure~\ref{figure:interaction} shows the typical usage of \code{cluster()}
to cluster
data from a DSD object into a DSC object and then obtaining the clustering
via \code{get_centers()}.

\begin{figure} 
\centering 
\includegraphics[width=6cm]{interaction} 
\caption{Interaction between the DSD and DSC classes} 
\label{figure:interaction} 
\end{figure}

The implementations for DSC are split again into R-based implementations
(\code{DSC_R}), traditional macro-based implementations (\code{DSC_Macro}) and MOA-based implementations (\code{DSC_MOA}).
Currently most available algorithms 
use MOA implementations.
\begin{itemize}
\item
StreamKM++~\citep{stream:Ackermann+Lammersen+Maertens+Raupach:2010}
\item
ClusTree~\citep{stream:Kranen+Assent+Baldauf+Seidl:2009}
\item
DenStream~\citep{stream:Cao+Ester+Qian+Zhou:2006}
\item
CluStream~\citep{stream:Aggarwal+Han+Wang+Yu:2003}
\item
CobWeb~\citep{stream:Fisher:1987}
\end{itemize}

The only R implementation
is threshold Nearest
Neighbor~\citep{stream:Hahsler+Dunham:2010,stream:Hahsler+Dunham:2010b}
as class \code{DSC_tNN}.

There are three traditional clustering algorithms that can be compared to data stream clustering methods or can be used to recluster results.

\begin{itemize}
\item
Kmeans
\item
DBSCAN
\item
Hierarchical
\end{itemize}

\section{Extending the \pkg{stream} Framework} \label{sec:design:extension}

Since stream mining is a relatively young field and many advances are
expected in the near future,
the object oriented framework in \pkg{stream} is developed with easy 
extensibility in mind. Implementations for data streams (DSD) and
data stream tasks (DST) can be easily added by implementing a small
number of core functions. The actual implementation can be written 
in either \proglang{R}, \proglang{Java},
\proglang{C}/\proglang{C++} or any other programming language
which can be interfaced by \proglang{R}.
In the following we discuss the extension of DSD and DST.

\subsection{Implementing new Data Stream Data Classes}

The class hierarchy in Figure~\ref{figure:dsd} is implemented 
following the S3 class system by using a vector
of class names for the class attribute. For example, an object of
class \code{DSD_Gaussian_Static} will have the class attribute
\code{c("DSD_Gaussian_Static", "DSD_R", "DSD")} indicating that
the object also is an R implementation of DSD. This allows 
the framework to implement all common functionality as functions at the level
of \code{DSD} and \code{DSD_R} and only a minimal set of functions
has to be implemented in order to add a new data stream implementation.

For a new implementation only a creator function and \code{get_points()}
needs to be implemented.
The creator function creates an object of the appropriate
\code{DSD} subclass. Typically this S3 object is a list of all parameters,
an open R connection and/or an environment for storing
state information (e.g., the current position in the stream).
Also an element called \code{"description"} should be provided. This element
is used by \code{print()}.
Note that the class attribute has to contain a vector of all parent classes
in the class diagram in bottom-up order.
The implemented \code{get_points()} needs to dispatch for the class
and create as the output a data.frame containing the data points as
rows. 

%% FIXME: talk about assignement attribute.

\subsection{Implementing new Data Stream Task Classes}

We concentrate again on data stream clustering. However,
to add new data stream mining tasks, a subclass hierarchy 
similar to the hierarchy in Figure~\ref{figure:dst} for data stream
clustering (DSC) can be easily added.

To implement a new clustering algorithm, a creator function 
(typically named after the algorithm) and a \code{get_clustering()} function
is needed. The clustering algorithm itself is part 
of the object created by the creator. To understand this slightly complicated
approach consider again Figure~\ref{figure:interaction}.
The framework provides 
the function \code{cluster(dsc, dsd, n=1)} 
which contains a loop to go through \code{n} new data points.
In the loop a single data points is obtained from \code{dsd} using
its \code{get_point()} function and then the data point is passed on to an
internal generic clustering function which has implementations 
for \code{DSC_MOA} and \code{DSC_R}. The implementation for
\code{DSC_MOA} takes care of all MOA-based clustering algorithms.
For R-based implementation the \code{DSC_R} version looks in
the list of the \code{dsc} object 
for an element called \code{"clusterFun"}
containing a function. This function accepts two arguments, 
the DSC object and a data point to be added to the DSC object.
In order for this to work,
DSC objects need to 
contain the clustering function and also
be mutable which means that the object can be changed
without creating a copy and assigning it back to the object name. 
Mutability was introduces recently to R in the form of reference classes. 
However, different to regular function calls, 
reference classes use a completely differnt object-oriented
method invocation style which would confuse users of \pkg{stream}.
We therefore decided to hide the implementation by wrapping
it into a regular S3 DSC object which now contains a mutable clustering.
This way a call of the from
\code{cluster(dsc, dsd, 1)} will 
instruct the reference class object to update its clustering resulting in a 
changed \code{dsc}.

To obtain the clustering result a function \code{get_centers()} which
dispatched for the new class needs to be implemented.
This function extracts the centers of the clusters from the 
reference class object in \code{dsc} and returns them as a data.frame.

\section{Examples} \label{sec:examples}

Developing new data stream mining algorithms and 
comparing them experimentally is the main purpose of
\pkg{stream}. In this section we give several 
increasingly complex
examples of how to use \pkg{stream}. 
First, we start with creating a data stream using 
different implementations of the DSD class.
The second example shows how to save and read stream data to and from disk.
We then give examples in how to reuse the same data from a stream
in order to perform comperison experiments mith multiple data stream mining
algorithms.
Finally, the last example introduces the use of data stream clustering
algorithms with a detailed
comparison of two algorithms from start to finish by first running the online
components on the same data stream, then using a $k$-means algorithm 
to re-cluster the
micro-clusters generated by each algorithm into final clusters.

\subsection{Creating a data stream} \label{examples:ds}

In this example, we focus on the DSD class to model
data streams.

<<>>= 
library("stream") 
set.seed(1000) 

dsd <- DSD_Gaussian_Static(k=3, d=2, noise=0.05) 
dsd 
@

After loading the \pkg{stream} package and setting a seed for the random
number generator, we call the creator function for the class 
\code{DSD_Gaussian_Static} specifying the number of clusters as $k=4$,
the data dimensionality to $d=2$ and a noise of 5\%.
This data stream generator chooses for each cluster randomly a mean
and a covariance matrix. 

New data points are requested from the stream using \code{get_points()}.
When a new data point is requested from  this generator, 
a cluster is chosen randomly and then point is drawn from
a multivariate normal distribution given by the mean and covariance matrix of
the cluster.
In the following instruction requests $n=5$ new data points.

<<>>= 
data <- get_points(dsd, n=5) 
data 
@

The result is a data.frame containing the data points as rows. For evaluation
it is often important to know the ground truth, in this case from which
cluster each point was created. The generator also returns the ground 
truth if it is called with \code{assignment=TRUE}.

<<>>= 
data <- get_points(dsd, n=5, assignment=TRUE) 
data 
@

The ground truth is returned as an attribute with the name 
\code{"assignment"} and can easily be accessed in the following way:

<<>>= 
attr(data, "assignment") 
@

Note that we created a generator with 5\% noise. Noise points
do not belong to any cluster and thus
have an assignement value of \code{NA}.

Next, we plot 500 points from the data stream to get an idea about its 
structure.

<<static, fig=TRUE, include=FALSE>>= 
data <- get_points(dsd, n=500, assignment=TRUE)
cl <- attr(data, "assignment")
plot(data, col=cl, pch=cl) 
points(data[is.na(cl),], col="gray")
@

\begin{figure} 
\centering 
\includegraphics[width=.5\linewidth]{stream-static}
\caption{Plotting 1000 data points from the data stream} 
\label{figure:static}
\end{figure}

Figure~\ref{figure:static} shows 
the resulting plot. 
The assignment value is used to change the color and the point type in the 
plot. Since noise points have an assignemnt value of \code{NA} they are
not plotted by the \code{plot} and \code{points} is used to add them
to the plot.

\code{DSD_RandomRBFGeneratorEvents} creates dynamic data streams where clusters move over time.

<<moa1, fig=TRUE, include=FALSE>>= 
set.seed(1000) 
dsd <- DSD_RandomRBFGeneratorEvents(k=3, d=2) 
dsd
@

$k$ and $d$ again represent the number of clusters and the dimensionality
of the data, respectively. 
In the following we request 4 times 500 data points from the stream and 
create a plot. 

<<eval=FALSE>>=
plot(get_points(dsd, 500)) 
plot(get_points(dsd, 500)) 
plot(get_points(dsd, 500)) 
plot(get_points(dsd, 500)) 
@

<<moa1, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(get_points(dsd, 500)) 
@
<<moa2, fig=TRUE, include=FALSE, echo=FALSE>>= 
plot(get_points(dsd, 500)) 
@
<<moa3, fig=TRUE, include=FALSE, echo=FALSE>>= 
plot(get_points(dsd, 500)) 
@
<<moa4, fig=TRUE, include=FALSE, echo=FALSE>>= 
plot(get_points(dsd, 500)) 
@

\begin{figure} 
\begin{minipage}{.48\linewidth} \centering
\includegraphics[width=\linewidth]{stream-moa1} \\(a) 
\end{minipage}
\begin{minipage}{.48\linewidth} \centering
\includegraphics[width=\linewidth]{stream-moa2} \\(b) 
\end{minipage} \\
\begin{minipage}{.48\linewidth} \centering
\includegraphics[width=\linewidth]{stream-moa3} \\(c) 
\end{minipage}
\begin{minipage}{.48\linewidth} \centering
\includegraphics[width=\linewidth]{stream-moa4} \\(d) 
\end{minipage}
\caption{The concept drift of DSD\_MOA} 
\label{figure:moa} 
\end{figure}

Figure~\ref{figure:moa} shows the 
results of the four plot instructions.
It shows that the 3 clusters move over time.


\subsection{Reading and writing data streams} \label{examples:disk}

Altough data streams by definition are unbounded and thus
storing them long term is typially infeasible, it is often useful
to store parts of a stream to disk. For example, a small part
of a stream with an interesting feature can be used to test 
how a new algorithm handles this specific case.
\pkg{stream} has support for
reading and writing parts of data streams 
through an R connection which provide a set of 
functions to interface file-like objects like files, compressed files,
pipes, URLs or sockets~\citep{stream:RIO:1111}.

We start by creating a DSD object. 

<<echo=FALSE>>= 
library("stream") 
set.seed(1000) 
dsd <- DSD_Gaussian_Static(k=3, d=5) 
dsd
@

Next, we write 100 data points to disk using \code{write_stream()}.

<<>>= 
write_stream(dsd, "dsd_data.csv", n=100, sep=",") 
@

\code{write_stream()} accepts 
a DSD object, and then 
either a connection directly, or the file name.
The instruction above will create a new file called
\code{dsd\_data.cvs} (an existing file will be overwritten). 
The \code{sep} parameter defines how the dimensions in each 
data point (row) are separated. 
Here \code{","} is used to create a comma separated values file.
The actual writing is done by 
the \code{write.table()} function and any additional parameters are passed
directly to it. Data points are requested individually from the stream and
then written to the connection. This way the only restriction for the
size of the written stream is the available storage at the receiving end.

The \code{DSD_ReadStream} object is used to read a stream from
a connection or a file.
\code{DSD_ReadStream} works in a way similar to \code{write_stream()} in that
it reads a single data point at a time with the \code{read.table()} function.
If the read in data is processed, e.g., by a data stream clustering
algorithm, and then removed from memory, 
we can process files larger than the available main memory.

<<>>= 
dsd_file <- DSD_ReadStream("dsd_data.csv", sep=",") 
dsd_file
@

\code{DSD_ReadStream} objects are just like any other DSD object in that you
can call \code{get_points()} to retrieve data points from the data stream.

<<>>=
get_points(dsd_file,5)
@


Multiple loops and reseting a \code{DSD_ReadStream} is possible
and will described in the next example.

\subsection{Replaying a data stream} \label{examples:replay}

An important feature of \pkg{stream} is the ability to replay portions of a 
data stream. With this feature we can capture a special feature of the
data (e.g., an anomality) and then change our algorithm and test if the 
change improved the behavior on exactly that data.
Also the feature can be used to
conduct experiments where different algorithms need to 
see exactly the same data.

There are several ways to replay streams. We can write a portion of a stream to
disk with \code{write_stream()} and then use \code{DSD_ReadStream} to read the
stream portion back every time it is needed.
However, often the interesting portion of the stream is small enought to
fit into memeory or might be already available as a matrix or a data.frame in
R. In this case we can use the DSD class \code{DSD_Wrapper} which
provides a stream interface for a matrix/data.frame.

First we create some data and use \code{get_points()} to store 100
points as a data.frame in \code{points}.
<<>>= 
library("stream") 
set.seed(1000) 
dsd <- DSD_Gaussian_Static(k=3, d=2) 
points <- get_points(dsd, 100)
head(points)
@

Next, we create a \code{DSD_Wrapper} object with the points.

<<>>= 
replayer <- DSD_Wrapper(points) 
replayer 
@

Every time we get a point from replayer it moves to the next position.

<<>>=
get_points(replayer, n=5)
replayer
@

The stream only has 99 points left and requestion more will result in
an error.

<<eval=FALSE>>=
get_points(replayer,n = 1000)
@

\code{DSD_Wrapper} and \code{DSD_ReadStream} can be created to loop 
indefinately, i.e., start over once they reach the last data point.
This is achived by passing \code{loop=TRUE} to the creator function.
These two types of DSD classes can start over with the first 
data point by using \code{reset_stream()}.

<<>>=
reset_stream(replayer)
replayer
@

\subsection{Clustering a data stream} \label{examples:clustering_ds}

In this example we show how to cluster data using DSC objects.  
First, we create a data stream (two Gaussian clusters in two dimesions
with 5\% noise).

<<>>= 
library("stream") 
set.seed(1000) 
dsd <- DSD_Gaussian_Static(k=3, d=2, noise=0.05)
dsd
@

Next, we prepare the clustering algorithm. We use here \code{DenStream}
and set the number of points used for initialisation to 100.

<<>>=
dsc <- DSC_DenStream(initPoints = 100) 
dsc
@

Now we are ready to cluster data from the stream using
the \code{cluster()} function. Note, that \code{cluster()}
will implicitly alter \code{dsc} so no
reassignment is necessary.

<<>>= 
cluster(dsc, dsd, 500) 
dsc
@

After clustering 500 data points data the clustering contains
\Sexpr{nclusters(dsc)} micro clusters. The micro cluster centers
are:
<<>>=
get_centers(dsc)
@


<<cluster, fig=TRUE, include=FALSE>>= 
plot(get_points(dsd,500), col="grey", pch=3, main = "DenStream")
points(get_centers(dsc), col="red", cex=5)
@

\begin{figure} \centering \includegraphics[width=.5\linewidth]{stream-cluster}
\caption{Plotting the micro-clusters on top of data points}
\label{figure:cluster} \end{figure}

Figure~\ref{figure:cluster} is the result of the calls to \code{plot()} and
\code{points()}. It shows the micro-clusters as red crosses on top of grey data
points. It is often helpful to visualize the results of the clustering
operation during the comparison of algorithms.

\subsection{Evaluating results} \label{examples:evaluation}

In this example we will show how to display evaluation measures after clustering
data using a DSC object with the \code{get_evaluation()} function.

There are various scores that can be measured using the \code{get_evaluation()} function.
The complete list of measures includes: f1, recall, precision, number of cluster,
numClasses, ssq, and rand.

To use the function simply pass in the \code{DSC} object as well as a \code{DSD}
object to test the quality of the clustering algorithm.

<<>>= 
library("stream")

dsd <- DSD_Gaussian_Static(k=3, d=2, mu=rbind(c(1.5,1.3),c(1,1),c(1.2,1)))

dsc <- DSC_tNN(, threshold=.1)
cluster(dsc,dsd,500)

get_evaluation(dsc,dsd, method = "f1", n = 500)


get_evaluation( dsc,dsd, method = "recall", n = 500)
@

This produces the f1 score and the recall of the \code{DSD} object. The other evaluation
measures can be called in a similar fashion.


\subsection{Reclustering DSC objects with another DSC} \label{examples:recluster}

This examples show how to recluster a \code{DSC} object after creating it. This example
will also show how to utilize the \code{DSC_Wrapper} in conjunction with the reclustering
function in order to run and evaluate classical models like kmeans as a \code{DSC} object.

To begin, first create a \code{DSC} object and run the clustering algorithm.

<<>>= 
library("stream")

dsd <- DSD_Gaussian_Static(k=3, d=2, mu=rbind(c(1.5,1.3),c(1,1),c(1.2,1)))

dsc <- DSC_tNN(, threshold=.1)
cluster(dsc,dsd,1000)
@

Next, you will want to grab the centers of your microclusters and then wrap them as a DSD.
This will allow you to insert your microclusters into another clustering algorithm. The supported
macro clustering models that are typically used for reclustering are kmeans, hierarchial, and dbscan.



After running the \code{cluster()} function, your reclustering DSC will then have the standard
clustering information. This information can later be used to evaluate the performance of the algorithm.



In this example the f1 score is computed for the reclustered data.


\subsection{Full experimental comparison} \label{examples:full}

This example shows the \pkg{stream} framework being used from start to finish.
It encompasses the creation of data streams, data clusterers, the online
clustering of data points as micro-clusters, and then the comparison of the
offline clustering of 2 data stream clustering algorithms by applying the
\textit{k-means} algorithm. As such, less detail will be given in the topics
already covered in the previous examples and more detail will be given on the
comparison of the 2 data stream clustering algorithms.

Setting up the experiment:

<<echo =FALSE>>= 
set.seed(1000) 
@

<<>>= 
library("stream") 
d <- get_points(DSD_Gaussian_Static(k=3, d=2, noise=0.01), 10000) 
head(d)

dsd <- DSD_Wrapper(d, k=3) 
dsd

dsc1 <- DSC_DenStream() 
dsc2 <- DSC_CluStream() 
@

Clustering the data:

<<>>= 
cluster(dsc1, dsd, 10000) 
reset_stream(dsd) 
cluster(dsc2, dsd, 10000)
dsc1 
dsc2 
@

Now we plot the data and the 2 sets of micro-clusters generated.

<<clustream, fig=TRUE, include=FALSE>>= 
plot(d, xlab='x', ylab='y', col='grey', pch=4, cex=.5) 
points(get_centers(dsc2), col='blue', cex=2, lwd=2) 
@

<<denstream, fig=TRUE, include=FALSE>>= 
plot(d, xlab='x', ylab='y', col='grey', pch=4, cex=.5) 
points(get_centers(dsc1), col='red', cex=2, lwd=2) 
@

\begin{figure} 
\begin{minipage}[b]{.48\linewidth} \centering
\includegraphics{stream-denstream} \\(a) DenStream 
\end{minipage}
\begin{minipage}[b]{.48\linewidth} \centering
\includegraphics{stream-clustream} \\(b) CluStream 
\end{minipage}
\caption{Plotting 2 sets of different micro-clusters 
against the generated data} 
\label{figure:denstream+clustream} 
\end{figure}

The code above creates a \code{DSD_Wrapper} object from a
\code{DSD_Gaussian_Static} object so that we can replay the same stream data
for both DSC objects. We then use the \code{DSD_Wrapper} to feed the exact data
stream into 2 different algorithms, DenStream and CluStream, during the
\code{cluster()} operation. Note that after each call to \code{cluster()},
we also have to call \code{reset_stream()} to reset the \code{DSD_Wrapper}
back to its original position.


After the clustering operations, we plot the calculated micro-clusters and the
original data. Figure~\ref{figure:denstream+clustream} shows the 2 sets of
micro-clusters, in red and blue, over the original data which is in grey. We
have plotted the micro-clusters as circles to more closely reflect their
nature, however, the circles are merely a representation and the radii haven't
been calculated specifically for each micro-cluster. The plot makes it easy to
point out differences in the two algorithms. The DenStream micro-clusters, in
red, stay true to the nature of the algorithm in that they congregate where
there is a large number of data points, or in other words, dense areas.
CluStream on the other hand, in blue, is more evenly spread, and the
micro-clusters are relatively separated, covering most of the area that the
generated data fills.

<<denstream+kmeans, fig=TRUE, include=FALSE>>= 
plot(d, xlab='x', ylab='y', pch=4, cex=.5, col='grey') 
points(kmeans(get_centers(dsc1), centers=3, nstart=5)$centers,
    col='red', cex=14, lwd=2) 
@

<<clustream+kmeans, fig=TRUE, include=FALSE>>= 
plot(d, xlab='x', ylab='y', pch=4, cex=.5, col='grey') 
points(kmeans(get_centers(dsc2), centers=3, nstart=5)$centers,
    col='blue', cex=14, lwd=2) 
@

We can then take this a step further.
Figure~\ref{figure:denstream+clustream+kmeans} shows a new plot---in this case,
we are plotting the calculated macro-clusters of each algorithm as a result of
a \textit{k-means} operation. We use the term ``macro'' here to differentiate
the \textit{k-means} clusters from the micro-clusters generated by the stream
clustering algorithms. Again, the DenStream clusters are shown in red, and the
CluStream clusters are shown in blue. We have enlarged the circle
representations for the \textit{k-means} clusters to better show the area they
cover.


This last operation is an example of how we use the same offline component for
two different algorithms, and the differences that it produces. \proglang{R}
contains an assortment of traditional clustering algorithms that are available
through the installation of various packages. It is up to the user to decide
which clustering algorithm they would like to use as the offline component.
Most stream clustering algorithms are developed with a certain offline
algorithm in mind, but it is interesting to see the different combinations of
algorithms and the results they produce.


There are several external packages that are required to use the \pkg{stream}
package. These include the \pkg{proxy} package, written by
\cite{stream:Meyer+Buchta:2010}, the \pkg{MASS} package by
\cite{stream:Venables+Ripley:2002}, and \pkg{clusterGeneration} by
\cite{stream:Qiu+Joe:2009}. To facilitate the communication between
\proglang{R} and \proglang{Java}, we used the \pkg{rJava} package by
\cite{stream:Urbanek:2010}. This allows us to make method calls directly to the
JRI from within the \proglang{R} environment.

\begin{figure} \begin{minipage}[b]{.48\linewidth} \centering
\includegraphics{stream-denstreamkmeans} \\(a) 
\end{minipage}
\begin{minipage}[b]{.48\linewidth} \centering
\includegraphics{stream-clustreamkmeans} \\(b) 
\end{minipage}
\caption{Plotting the results of a \textit{k-means} operation on each stream
clustering algorithm} 
\label{figure:denstream+clustream+kmeans} 
\end{figure}

\subsection{Batch processing of clustering algorithms} \label{examples:batch}

It may be of use to perform a batch job on multiple clusters to evaluate and compare them quickly. The \code{batch_Cluster()} function is used to perform this operation. As inputs, it accepts a list of DSC's, a common DSD for creating the clusters, an optional DSD used for testing the clusters, a list of scores to solve for, a list of reclustering DSC's, and parameters for the DSD size. Below is an example:



This block of code effectively uses the first 1000 points of the \code{DSD} to construct clusters using both the \code{DSC_DenStream} and the \code{DSC_CluStream} algorithms. It also will recluster both results using \textit{k-means} and will return the evaluation scores of each clustering combination.

\section{Conclusion and Future Work} \label{sec:conclusion}

\pkg{stream} is a data stream modeling framework in \proglang{R} that has both
a variety of data stream generation tools as well as a component for performing
data stream mining tasks. The flexibility offered by our framework allows the
user to create a multitude of easily reproducible experiments to compare the
performance of these tasks. Data streams can be created with specific
properties that may be difficult to simulate in real-world situations.


Furthermore, the infrastructure that we have built can be extended upon in
multiple directions. We have abstracted each component to only require a small
set of functions that are defined in each base class. Writing the framework in
\proglang{R} means that developers have the ability to design components either
directly in \proglang{R}, or design components in \proglang{Java} or
\proglang{C}/\proglang{C++}, and then write an \proglang{R} wrapper to use the
high level code. Upon completion, stream will be available from The
Comprehensive R Archive Network (CRAN) website for download
\citep{stream:cran:2010}.


In the future, we plan on adding additional functionality to \pkg{stream}.
Currently we only have implementations for clustering tasks; we would like to
develop a classification module that also extends from the base DST class.
Additionally, there are plans to develop an evaluation module that accompanies
each DST class to provide immediate feedback on their performance. Finally, for
each of the DST classes developed, we would like to include all of the
available algorithms, both the latest innovations and the original algorithms
that shaped the research for the respective area.


\bibliography{stream,stream_clust}

%%\appendix %\section{stream Reference Manual}
%%\includepdf[pages=-]{manual.pdf}

\end{document}
